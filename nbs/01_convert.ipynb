{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert\n",
    "> Script to automatically convert several files (created on October 19th 2022 by M. Reichmann (remichae@phys.ethz.ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/00\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "from HighResAnalysis.src.run import init_batch\n",
    "from HighResAnalysis.plotting.utils import choose, info, colored, GREEN, RED, check_call, critical, warning\n",
    "from HighResAnalysis.utility.utils import print_banner, PBAR, small_banner, byte2str\n",
    "from HighResAnalysis.src.analysis import Analysis\n",
    "from HighResAnalysis.src.converter import Converter\n",
    "from HighResAnalysis.cern.converter import CERNConverter\n",
    "from HighResAnalysis.src.converter import batch_converter, Converter\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from numpy import where, diff, array, concatenate, cumsum, append, mean\n",
    "from functools import partial\n",
    "from fastcore.script import *\n",
    "from subprocess import check_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoConvert:\n",
    "\n",
    "    Force = False\n",
    "\n",
    "    def __init__(self, first_run=None, last_run=None, batch=None, beamtest=None, verbose=False, force=False):\n",
    "\n",
    "        self.Ana = Analysis(beamtest, verbose=verbose)\n",
    "        self.Batch = init_batch(batch, dut=0, beam_test=self.Ana.BeamTest)\n",
    "        self.Converter = CERNConverter if self.Ana.BeamTest.Location == 'CERN' else Converter\n",
    "\n",
    "        self.FirstRun, self.LastRun = first_run, last_run\n",
    "        AutoConvert.Force = force\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Converter for {self.Batch!r}'\n",
    "\n",
    "    @property\n",
    "    def converters(self):\n",
    "        return [self.Converter.from_run(r) for r in self.Batch.Runs if (not r.FileName.exists() or self.Force) and self.first_run <= r <= self.last_run]\n",
    "\n",
    "    @property\n",
    "    def final_files_exist(self):\n",
    "        return all([r.FileName.exists() and self.first_run <= r <= self.last_run for r in self.Batch.Runs])\n",
    "\n",
    "    @property\n",
    "    def first_run(self):\n",
    "        return choose(self.FirstRun, self.Batch.min_run)\n",
    "\n",
    "    @property\n",
    "    def last_run(self):\n",
    "        return choose(self.LastRun, self.Batch.max_run)\n",
    "\n",
    "    def copy_raw_files(self):\n",
    "        conv = self.converters\n",
    "        n_missing = sum([not f.exists() for c in conv for f in c.raw_files])\n",
    "        if n_missing > 0:\n",
    "            info(f'try to copy {n_missing} raw files from {Analysis.Config.get(\"data\", \"server\")} ...')\n",
    "            PBAR.start(n_missing)\n",
    "            for c in conv:\n",
    "                for f in c.raw_files:\n",
    "                    if c.download_raw_file(f, out=False) is not None:\n",
    "                        PBAR.update()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_run(c: Converter):\n",
    "        c.run(force=AutoConvert.Force)\n",
    "        return c\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"parallel conversion\"\"\"\n",
    "        conv = self.converters\n",
    "        if not conv:\n",
    "            return True\n",
    "        self.copy_raw_files()\n",
    "        info(f'Creating pool with {cpu_count()} processes')\n",
    "        with Pool() as pool:\n",
    "            result = pool.map_async(self.convert_run, conv)\n",
    "            conv = result.get()\n",
    "            small_banner('Summary:')\n",
    "            for c in conv:\n",
    "                speed = f'{c.Run.n_ev}, {c.Run.n_ev / c.T1.total_seconds():1.0f} Events/s' if c.finished else 'NOT CONVERTED!'\n",
    "                print(colored(f'{c} --> {str(c.T1)[:-5]} ({speed})', GREEN if c.finished else RED))\n",
    "            return all([c.finished for c in conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BatchConvert(AutoConvert):\n",
    "\n",
    "    def __init__(self, batch=None, beamtest=None, verbose=False, force=False):\n",
    "        super().__init__(None, None, batch, beamtest, verbose, force)\n",
    "        self.Converters = [self.Converter.from_run(r, dut_name=self.Batch.DUTName) for r in self.Batch.Runs]\n",
    "        self.Converter = batch_converter(self.Converters[0].__class__).from_batch(self.Batch)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_run(c: Converter):\n",
    "        f, out = c.Proteus.Steps[-1]\n",
    "        c.run(force=True, steps=c.first_steps + [(partial(f, progress=False), out)], rm=False)\n",
    "        return c\n",
    "\n",
    "    @property\n",
    "    def converters(self):\n",
    "        return list(filter(lambda c: not c.proteus_raw_file_path().exists() or not c.Proteus.OutFilePath.exists(), self.Converters))\n",
    "\n",
    "    def single(self):\n",
    "        return super().run()\n",
    "\n",
    "    def run(self):\n",
    "        if super().run():  # create root files of the single runs\n",
    "            self.merge_files()\n",
    "            self.Converter.run(force=self.Force, rm=False)  # tracking and hdf5 conversion of the single merged file\n",
    "            self.fix_event_nrs()\n",
    "            self.remove_aux_files()\n",
    "        else:\n",
    "            critical('Not all runs were converted ... please re-run')\n",
    "\n",
    "    @property\n",
    "    def file_size_ratios(self):\n",
    "        return array([c.Proteus.OutFilePath.stat().st_size / c.Run.n_ev for c in self.Converters])\n",
    "\n",
    "    def check_file_sizes(self):\n",
    "        r = self.file_size_ratios\n",
    "        bad_runs = array(self.Batch.Runs)[r < mean(r) * .9]\n",
    "        return True if bad_runs.size == 0 else bad_runs\n",
    "\n",
    "    def check_proteus_files(self, pl=0, branch='trk_v', min_run=0):\n",
    "        conv = [c for c in self.Converters if c.Run >= min_run]\n",
    "        PBAR.start(len(conv))\n",
    "        for c in conv:\n",
    "            with uproot.open(c.Proteus.OutFilePath) as f:\n",
    "                key = f.keys(recursive=False)[pl]\n",
    "                try:\n",
    "                    array(f[f'{key}/tracks_clusters_matched'][branch])\n",
    "                except Exception as err:\n",
    "                    warning(f'{c} has corrupted data ({err})')\n",
    "                PBAR.update()\n",
    "\n",
    "    def merge_proteus_files(self):\n",
    "        \"\"\"merge proteus out files of all batch runs\"\"\"\n",
    "        out = self.Converter.Proteus.OutFilePath\n",
    "        if not out.exists() or self.Force:\n",
    "            cmd = f'hadd -f {out} {\" \".join(str(c_.Proteus.OutFilePath) for c_ in self.Converters)}'\n",
    "            info(cmd)\n",
    "            check_call(cmd, shell=True)\n",
    "        else:\n",
    "            info(f'found {out}')\n",
    "\n",
    "    def merge_trigger_info_files(self):\n",
    "        \"\"\"merge dut files with trigger info for the CERN analysis\"\"\"\n",
    "        out = self.Converter.trigger_info_file()\n",
    "        if not out.exists() or self.Force:\n",
    "            cmd = f'hadd -f {out} {\" \".join(str(c_.trigger_info_file()) for c_ in self.Converters)}'\n",
    "            info(cmd)\n",
    "            check_call(cmd, shell=True)\n",
    "        else:\n",
    "            info(f'found {out}')\n",
    "\n",
    "    def merge_files(self):\n",
    "        self.merge_proteus_files()\n",
    "        if self.Converter.trigger_info_file() != self.Converter.proteus_raw_file_path():\n",
    "            self.merge_trigger_info_files()\n",
    "\n",
    "    def fix_event_nrs(self):\n",
    "        \"\"\"fix the events numbers in the merged hdf5 file\"\"\"\n",
    "        n_ev = [uproot.open(c.trigger_info_file())['Event'].num_entries for c in self.Converters]  # number of events for each run\n",
    "        with h5py.File(self.Batch.FileName, 'r+') as f:\n",
    "            ev = array(f['Tracks']['Events']).astype('i8')   # event number of the tracks which needs to be fixed\n",
    "            i_tr = diff(concatenate([[0], where(diff(ev) < 0)[0] + 1, [ev.size]]))  # number of tracks for each run\n",
    "            f['Tracks']['Events'][...] = ev + cumsum(append(0, n_ev[:-1])).repeat(i_tr).astype('u4')  # add the cumulated number of events from the previous runs\n",
    "\n",
    "    def remove_aux_files(self):\n",
    "        for conv in self.Converters:\n",
    "            conv.remove_aux_files()\n",
    "        self.Converter.remove_aux_files()\n",
    "\n",
    "    def remove_raw_files(self):\n",
    "        f = [i for c in self.Converters for i in c.raw_files if i.exists()]\n",
    "        info(f'removing {len(f)} raw files ({byte2str(sum([i.stat().st_size for i in f]))})')\n",
    "        for conv in self.Converters:\n",
    "            conv.remove_raw_files(warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(m:Param('turn parallel processing ON', action='store_true'),\n",
    "         v:Param('turn verbose OFF', action='store_false'),\n",
    "         t:Param('turn test mode ON', action='store_true'),\n",
    "         f:Param('force conversion', action='store_true'),\n",
    "         tc:str=None, # Test Campaign in YYYYMM format\n",
    "         s:int=None, # run number where to start, default [None], = stop if no end is provided\n",
    "         e:int=None, # run number where to stop, default [None]\n",
    "         b:str=None, # batch number, default [None]\n",
    "        ):\n",
    "\n",
    "#     from argparse import ArgumentParser\n",
    "\n",
    "#     parser = ArgumentParser()\n",
    "#     parser.add_argument('-m', action='store_true', help='turn parallel processing ON')\n",
    "#     parser.add_argument('-tc', nargs='?', default=None)\n",
    "#     parser.add_argument('s', nargs='?', default=None, help='run number where to start, default [None], = stop if no end is provided', type=int)\n",
    "#     parser.add_argument('e', nargs='?', default=None, help='run number where to stop, default [None]', type=int)\n",
    "#     parser.add_argument('-b', nargs='?', default=None, help='batch number, default [None]')\n",
    "#     parser.add_argument('-v', action='store_false', help='turn verbose OFF')\n",
    "#     parser.add_argument('-t', action='store_true', help='turn test mode ON')\n",
    "#     parser.add_argument('-f', action='store_true', help='force conversion')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    z = AutoConvert(s, e, b, tc, v, f) if s is not None else BatchConvert(b, tc, v, f)\n",
    "    a = z.Converter\n",
    "    cs = z.Converters if hasattr(z, 'Converters') else None\n",
    "\n",
    "    if not t:\n",
    "        cs = z.converters\n",
    "        if len(cs):\n",
    "            print_banner(f'Start converting runs {cs[0].Run} - {cs[-1].Run}', color=GREEN)\n",
    "            z.run()\n",
    "            print_banner('Finished Conversion!', color=GREEN)\n",
    "        else:\n",
    "            info('There is nothing to convert :-)\\n', blank_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp src.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "> run module that handles the files and information of a single run\n",
    "> created on October 5th 2018 by M. Reichmann (remichae@phys.ethz.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/00\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from numpy import where, roll, split, array\n",
    "from pathlib import Path\n",
    "from fastcore.script import *\n",
    "from HighResAnalysis.utility.utils import print_table, datetime, ev2str, remove_letters, Dir, small_banner, isint, do_pickle, file_hash\n",
    "from HighResAnalysis.plotting.utils import load_json, warning, critical\n",
    "from HighResAnalysis.src.analysis import Analysis, BeamTest\n",
    "from HighResAnalysis.utility.utils import choose\n",
    "from HighResAnalysis.src.dut import DUT\n",
    "from HighResAnalysis.src.spreadsheet import make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_runlog(p: Path):\n",
    "    f = p.joinpath(Analysis.Config.get('data', 'runlog file'))\n",
    "    if not f.exists():\n",
    "        warning('runlog file does not exist! -> creating new one!')\n",
    "        make(p.stem.replace('-', ''))\n",
    "    return load_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_nrs(p: Path):\n",
    "    log = load_runlog(p)\n",
    "    return [key for key, dic in log.items() if 'status' not in dic or dic['status'] == 'green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_batch(name, dut, beam_test: BeamTest, log=None):\n",
    "    batches = load_batches(beam_test)\n",
    "    return DUTBatch(name, beam_test, log) if type(batches[name]) == dict else Batch(name, dut, beam_test, log) if isint(dut) else Batch.from_dut_name(name, dut, beam_test, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_batches(bt: BeamTest, redo=False):\n",
    "    \"\"\" unify batches from run logs and custom batches and save them in a tmp file. \"\"\"\n",
    "    log_file = bt.Path.joinpath(Analysis.Config.get('data', 'runlog file'))\n",
    "    custom_file = Dir.joinpath('ensembles', f'{bt.Name}.json')\n",
    "    tmp_file = Analysis.MetaDir.joinpath('ensembles', f'{bt.Tag}.pickle')\n",
    "    tmp_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "    if tmp_file.exists() and not redo:\n",
    "        h1, h2, bs = do_pickle(tmp_file)\n",
    "        if file_hash(log_file) == h1 and file_hash(custom_file) == h2:\n",
    "            return bs\n",
    "\n",
    "    def load():\n",
    "        log = load_json(log_file)\n",
    "\n",
    "        # batches from log\n",
    "        d = array(sorted([(run, dic['batch']) for run, dic in log.items() if dic['status'] == 'green'], key=lambda x: x[1])).T\n",
    "        s = where(d[1] != roll(d[1], 1))[0]  # indices where the new batches start\n",
    "        batches = {batch: runs.astype('i8') for batch, runs in zip(d[1][s], split(d[0], s[1:]))}\n",
    "\n",
    "        # custom batches\n",
    "        if custom_file.exists():\n",
    "            good_runs = lambda runs: array([run for run in runs if log[str(run)]['status'] == 'green'])\n",
    "            batches.update({batch: {dut: good_runs(d_runs) for dut, d_runs in runs.items()} if type(runs) is dict else good_runs(runs) for batch, runs in load_json(custom_file).items()})\n",
    "\n",
    "        return file_hash(log_file), file_hash(custom_file), dict(sorted(batches.items(), key=lambda dic_pair: int(remove_letters(dic_pair[0]))))\n",
    "\n",
    "    return do_pickle(tmp_file, load, redo=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Batch:\n",
    "    \"\"\" class containing the run infos of a single batch. \"\"\"\n",
    "\n",
    "    DUTName = None\n",
    "\n",
    "    def __init__(self, name, dut_nr, beam_test: BeamTest, log=None):\n",
    "        self.Name = name\n",
    "        self.BeamTest = beam_test\n",
    "        self.DataDir = beam_test.Path\n",
    "        self.Log = load_runlog(self.DataDir) if log is None else log\n",
    "\n",
    "        self.Runs = self.load_runs(dut_nr)\n",
    "        self.FirstRun = self.Runs[0]\n",
    "        self.RunNrs = array([run.Number for run in self.Runs])\n",
    "        self.Size = len(self.Runs)\n",
    "        self.DUT = self.Runs[0].DUT\n",
    "\n",
    "        self.FileName = self.DataDir.joinpath('data', f'batch{self}.hdf5')\n",
    "\n",
    "        if not self.verify():\n",
    "            critical('the duts of the individual runs do not agree! This is not implemented yet ...')\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.Name)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} {self} ({self.Size} runs {self.min_run}-{self.max_run}, {ev2str(self.n_ev)} ev)'\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.Runs[item]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dut_name(cls, name, dut_name, bt: BeamTest, log=None):\n",
    "        log = load_runlog(bt.Path) if log is None else log\n",
    "        dut_nrs = list(set([log[str(nr)]['duts'].index(dut_name) for nr in load_batches(bt)[name]]))\n",
    "        if len(dut_nrs) == 1:\n",
    "            return cls(name, dut_nrs[0], bt, log)\n",
    "        critical(f'cannot instanciate Batch {name} from DUT {dut_name}, varying DUT numbers ...')\n",
    "\n",
    "    def verify(self):\n",
    "        if self.Name is None:\n",
    "            return True\n",
    "        duts = [run.DUTs for run in self.Runs]\n",
    "        return all([d == duts[0] for d in duts])\n",
    "\n",
    "    def load_runs(self, dut_nr):\n",
    "        batches = load_batches(self.BeamTest)\n",
    "        if self.Name in batches:\n",
    "            return [Run(nr, dut_nr, self.DataDir, log=self.Log) for nr in batches[self.Name]]\n",
    "        critical('unknown batch name')\n",
    "\n",
    "    @property\n",
    "    def n_ev(self):\n",
    "        return sum(run.n_ev for run in self.Runs)\n",
    "\n",
    "    @property\n",
    "    def min_run(self):\n",
    "        return self[0]\n",
    "\n",
    "    @property\n",
    "    def max_run(self):\n",
    "        return self[-1]\n",
    "\n",
    "    def show(self):\n",
    "        small_banner(f'{self!r}')\n",
    "        print_table([run.info for run in self.Runs], header=['Nr.', 'Events', 'DUTs', 'Begin', 'End'])\n",
    "\n",
    "    def show_all(self):\n",
    "        rows = []\n",
    "        log = {int(i): dic for i, dic in self.Log.items()}\n",
    "        for n, rs in load_batches(self.BeamTest).items():\n",
    "            duts, rs = (rs.keys(), list(rs.values())[0]) if type(rs) is dict else (log[rs[0]]['duts'], rs)\n",
    "            t_str = [f'{datetime.fromtimestamp(t)}'[-8:-3] for t in [log[rs[0]]['start'], log[rs[-1]]['end']]]\n",
    "            rows.append([n, f'{rs[0]:03d}-{rs[-1]:03d}', ev2str(sum([log[i]['events'] for i in rs])), ', '.join(duts)] + t_str)\n",
    "        print_table(rows, header=['Batch', 'Runs', 'Events', 'DUTs', 'Begin', 'End'])\n",
    "\n",
    "    def find_runs(self, dut, bias, min_run=0):\n",
    "        return array([run.Number for run in self.Runs if dut in run.DUTs and int(run.Info['hv'][run.DUTs.index(dut)]) == bias and run >= min_run])  # noqa\n",
    "\n",
    "    @staticmethod\n",
    "    def find_dut_numbers(batch_name, dut_name, log, bt):\n",
    "        new_batches = load_json(Dir.joinpath('ensembles', f'{bt.Path.stem}.json'))\n",
    "        logs = [log[str(run)] for run in new_batches[batch_name]] if batch_name in new_batches else filter(lambda x: x['batch'] == batch_name and x['status'] == 'green', log.values())\n",
    "        return [d['duts'].index(dut_name) for d in logs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DUTBatch(Batch):\n",
    "    \"\"\" extension of batch class for a single DUT (for runs with mismatching duts). \"\"\"\n",
    "\n",
    "    def __init__(self, name, beam_test: BeamTest, log=None):\n",
    "        super().__init__(name, 0, beam_test, log)\n",
    "        self.DUTName = self.DUT.Name\n",
    "\n",
    "    def verify(self):\n",
    "        return True\n",
    "\n",
    "    def load_runs(self, dut_nr=None):\n",
    "        batches = load_batches(self.BeamTest)\n",
    "        return [Run.from_dut_name(nr, dut_name, self.DataDir, self.Log) for dut_name, nrs in batches[self.Name].items() for nr in nrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Run:\n",
    "    \"\"\" Run class containing all the information for a single run from the tree and the json file. \"\"\"\n",
    "\n",
    "    def __init__(self, run_number, dut_number, tc_dir: Path, log=None):\n",
    "\n",
    "        # Main Info\n",
    "        self.Number = int(run_number)\n",
    "        self.TCDir = tc_dir\n",
    "\n",
    "        # Files\n",
    "        self.FileName = self.TCDir.joinpath('data', f'run{self.Number:04d}.hdf5')\n",
    "\n",
    "        # Info\n",
    "        self.Info = self.load_info(log)\n",
    "        self.DUTs = self.Info['duts']\n",
    "        self.NDUTs = len(self.DUTs)\n",
    "        self.Positions = self.Info['dut position']  # which of the DUT slots are occupied\n",
    "        self.DUT = DUT(dut_number, self.Info, has_ref=tc_dir.stem in Analysis.Config.get('REF', 'dates'))\n",
    "\n",
    "        # Times for the run log\n",
    "        self.LogStart = self.Info['start']\n",
    "        self.LogEnd = self.Info['end']\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.Number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Run {self} ({ev2str(self.n_ev)} ev)'\n",
    "\n",
    "    def __format__(self, format_spec):\n",
    "        return f'{self.Number:{format_spec}}'\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.Number <= (other.Number if isinstance(other, Run) else other)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.Number < (other.Number if isinstance(other, Run) else other)\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return self.Number >= (other.Number if isinstance(other, Run) else other)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.Number > (other.Number if isinstance(other, Run) else other)\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        return [str(self.Number), ev2str(self.n_ev), ', '.join(self.DUTs)] + [f'{datetime.fromtimestamp(t)}'[-8:-3] for t in [self.LogStart, self.LogEnd]]\n",
    "\n",
    "    def load_info(self, log=None) -> dict:\n",
    "        return (load_runlog(self.TCDir) if log is None else log)[str(self)]\n",
    "\n",
    "    def print_info(self):\n",
    "        print(f'{self!r}')\n",
    "        print_table(rows=[[key, str(datetime.fromtimestamp(value) if key in ['start', 'end'] else value)] for key, value in self.Info.items()])\n",
    "\n",
    "    @classmethod\n",
    "    def from_ana(cls, run_number, dut=0, ana: Analysis = None):\n",
    "        ana = choose(ana, Analysis)\n",
    "        return cls(run_number, dut, ana.BeamTest.Path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dut_name(cls, run_number, dut_name, tc_dir, log):\n",
    "        return cls(run_number, log[str(run_number)]['duts'].index(dut_name), tc_dir, log)\n",
    "\n",
    "    @property\n",
    "    def n_ev(self):\n",
    "        return self.Info['events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Ensemble(object):\n",
    "    \"\"\" General enseble class for runs and batches. \"\"\"\n",
    "\n",
    "    FilePath = Dir.joinpath('ensembles', 'scans.json')\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "\n",
    "        self.Name = name\n",
    "        self.Dic = self.load_dic()\n",
    "        self.DUTName = self.Dic.pop('dut')\n",
    "        self.BeamTests = {bt: Analysis.load_test_campaign(bt) for bt in self.Dic}\n",
    "        self.Logs = {bt: load_runlog(Analysis.load_test_campaign(bt).Path) for bt in self.Dic}\n",
    "        self.Units = self.init_units()\n",
    "        self.Size = len(self.Units)\n",
    "        self.DUT = self.Units[0].DUT\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.Units[item]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.Name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} {self.Name} with {self.Size} units'\n",
    "\n",
    "    def load_dic(self):\n",
    "        d = load_json(Ensemble.FilePath)\n",
    "        if self.Name not in d:\n",
    "            critical(f'could not find ensemble \"{self.Name}\" in {self.FilePath}')\n",
    "        return d[self.Name]\n",
    "\n",
    "    def init_units(self):\n",
    "        return [self.init_run(n, bt) if isint(n) else self.init_batch(n, bt) for bt in self.Dic for n in self.Dic[bt]]\n",
    "\n",
    "    def init_run(self, run_number, bt):\n",
    "        return Run(run_number, self.Logs[bt][str(run_number)]['duts'].index(self.DUTName), self.BeamTests[bt].Path, self.Logs[bt])\n",
    "\n",
    "    def init_batch(self, name, bt):\n",
    "        return init_batch(name, self.DUTName, self.BeamTests[bt], self.Logs[bt])\n",
    "\n",
    "    @property\n",
    "    def biases(self):\n",
    "        return [u.DUT.Bias for u in self.Units]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def main(a:Param(action='store_true'), # show all\n",
    "        alle:Param(action='store_true'),\n",
    "        b:str='1a' # batch name\n",
    "        ):\n",
    "\n",
    "    # from argparse import ArgumentParser\n",
    "    # p_ = ArgumentParser()\n",
    "    # p_.add_argument('b', nargs='?', default='1a')\n",
    "    # p_.add_argument('-a', action='store_true')\n",
    "    # p_.add_argument('-all', action='store_true')\n",
    "    # args = p_.parse_args()\n",
    "\n",
    "    beam_tests = [Analysis.load_test_campaign(str(bt)) for ls in Analysis.Locations.values() for bt in ls]\n",
    "\n",
    "    a = Analysis()\n",
    "    b = init_batch(None if alle else b, 0, a.BeamTest)\n",
    "    r = b[0]\n",
    "    if a:\n",
    "        b.show_all()\n",
    "    else:\n",
    "        b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
